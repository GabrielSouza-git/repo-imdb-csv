#  Pipeline de ingest√£o IMDB com Apache Beam e Dataflow

Este projeto implementa um **pipeline de ingest√£o de dados** que l√™ um arquivo CSV do **Google Cloud Storage (GCS)**, transforma e carrega os dados no **BigQuery** usando o **Apache Beam** e o **Google Dataflow**.

---

##  Vis√£o geral

O objetivo √© automatizar a ingest√£o dos dados do dataset **IMDB Top 1000 Movies** (dispon√≠vel no Kaggle), utilizando ferramentas da **Google Cloud Platform (GCP)**.  
O pipeline executa as seguintes etapas:

1. L√™ um arquivo CSV armazenado no GCS.  
2. Transforma e padroniza os campos (tipos, nomes e formatos).  
3. Escreve o resultado final em uma tabela do BigQuery.  

---

##  Requisitos

- **Python 3.8+**
- **Bibliotecas:**
  ```bash
  pip install requirements.txt
  ```

---

### Entendendo o que vamos ingerir
O dataset utilizado ser√° esse: 
[Kaggle_IMDB_Dataset](https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows/data)

<div align="center">
<img width="1613" height="495" alt="Image" src="https://github.com/user-attachments/assets/6019a910-3af0-411d-99ae-423323346d19" />
</div>
Antes de sair codando, vale dar uma olhada no conte√∫do do arquivo pra entender a estrutura e a tipagem dos dados.  
Logo de cara, j√° d√° pra ver que algumas colunas merecem aten√ß√£o especial:

- **Poster_Link** ‚Üí traz uma URL gigante, ent√£o o tipo ideal √© `STRING`.  
- **Runtime** ‚Üí j√° vem em minutos e com escrita "min", ent√£o vale deixar como string e tratar esse campo pra facilitar ordena√ß√µes depois. 
- **Gross** ‚Üí vem com v√≠rgula nos valores e como o formato num√©rico padr√£o usa ponto, vamos precisar ajustar isso durante o processamento.  

Com isso em mente‚Ä¶ bora pra pr√°tica! üòé  

---

### Passo 1: Criar o bucket no Google Cloud Storage

Vamos come√ßar criando um bucket no **Google Cloud Storage** pra armazenar o arquivo que vamos ingerir.  
O nome do bucket precisa ser **√∫nico globalmente**, ent√£o escolha um que ainda n√£o exista.  
No meu caso, usei:  

```text
imdb-repo
```

<div align="center">
<img width="1238" height="809" alt="Image" src="https://github.com/user-attachments/assets/78cbe98b-e12c-4c84-a0be-de7e52e55624" />
</div>

### Passo 2: Preparando o ambiente no GCP

Com o bucket criado, o pr√≥ximo passo √© organizar nossos dados dentro dele.

Crie uma pasta chamada **`raw`** dentro do seu bucket, √© nela que vamos colocar o arquivo CSV que ser√° ingerido.  
<div align="center">
<img width="1038" height="454" alt="Image" src="https://github.com/user-attachments/assets/75404009-8d97-4387-bb8d-a4296d43afc5" />
</div>

\
Agora, fa√ßa o [imdb_top_1000.csv](https://github.com/GabrielSouza-git/repo-imdb-csv/blob/main/csv_dataflow_imdb.py) para dentro dessa pasta.  
<div align="center">
<img width="1658" height="364" alt="Image" src="https://github.com/user-attachments/assets/50148864-95cd-4749-bcc6-4c498f62afcb" />
</div>


### Passo 3: Criando o dataset no BigQuery

Com o arquivo salvo no GCS, bora preparar o destino dos dados no **BigQuery**.

1. V√° at√© o **BigQuery** no console do GCP.  
   Ao abrir, do lado esquerdo voc√™ ver√° o seu projeto listado.  
   Clique nos **tr√™s pontinhos** ao lado do nome do projeto e selecione **"Criar conjunto de dados"**.  
<div align="center">
<img width="1520" height="418" alt="Image" src="https://github.com/user-attachments/assets/e2867006-3055-401d-a012-cdbc0553504a" />
</div>


\
2. No menu que aparecer √† direita, defina o nome do seu dataset.  
   No meu caso, usei:  
   ```text
   imdb_dataset
   ```

<img width="528" height="662" alt="Image" src="https://github.com/user-attachments/assets/09bb44fa-dc87-4792-a3c2-e281c59054f9" />
</div>


\
3. Depois que criar, o dataset vai aparecer listado no painel esquerdo do BigQuery.
\
<img width="265" height="680" alt="Image" src="https://github.com/user-attachments/assets/500a4d08-8380-4056-9e50-a97f278cd4c6" />
</div>


### Passo 4: Configurando o ambiente virtual (venv)
Antes de executar o projeto, recomenda-se criar um ambiente virtual para isolar as depend√™ncias.
#### 4.1 - Criar o ambiente virtual
```bash
python -m venv venv
```
#### 4.2 - Ativar o ambiente virtual
```bash
source venv\Scripts\activate
```
#### 4.3 - Instalar as depend√™ncias
```bash
pip install -r requirements.txt
```

   
### Passo 5: Estrutura do c√≥digo
Com o bucket e o dataset prontos, chegou a hora de criar o c√≥digo que vai iniciar o job no Dataflow e fazer todo o processamento dos dados.
Agora sim √© a parte divertida üòé
C√≥digo j√° pronto: [csv_dataflow_imdb.py](https://github.com/GabrielSouza-git/repo-imdb-csv/blob/main/csv_dataflow_imdb.py)

#### 5.1 - Importa√ß√µes

O c√≥digo importa bibliotecas padr√£o do Python e m√≥dulos do Apache Beam:


<img width="748" height="276" alt="Image" src="https://github.com/user-attachments/assets/7a2e8c57-affc-4ca8-99d4-7bd6c10d3b7c" />
</div>


```text
- csv: leitura segura de linhas CSV.
- apache_beam: constr√≥i e executa o pipeline.
- ReadFromText: l√™ o arquivo CSV.
- WriteToBigQuery: grava os dados processados no BigQuery.
- BigQuery: API para acrescentar descri√ß√£o na tabela
```

#### 5.2 - Configura√ß√µes principais
Temos nossas constantes que centralizam as vari√°veis do projeto, servindo para facilitar a leitura e manuten√ß√£o do c√≥digo.

<img width="472" height="148" alt="Image" src="https://github.com/user-attachments/assets/09abf432-bdc6-42dc-9fd9-ca7889a3bdcf" />
</div>

```text
- GCS_INPUT = passamos o caminho onde o est√° nosso arquivo csv
- BQ_PROJECT = id do projeto
- BQ_DATASET = dataset que criamos anteriormente no BigQuery
- BQ_TABLE = nome que a tabela ter√° assim que criada
```

#### 5.3 - Estrutura e schema
Colunas esperadas do CSV

A fun√ß√£o **`parse_and_transform`** usa zip(CSV_COLUMNS, values), ent√£o a ordem das colunas no CSV deve ser exatamente igual √† lista CSV_COLUMNS, se n√£o, os dados ficar√£o desalinhados.

<img width="689" height="130" alt="Image" src="https://github.com/user-attachments/assets/485b576d-7961-4623-9802-8b3d296bcb67" />
</div>

Schema BigQuery

</div>
<img width="1012" height="511" alt="Image" src="https://github.com/user-attachments/assets/76c19333-3706-4eb4-8eed-90138d067f80" />
</div>

```text
- Define o esquema utilizado pelo WriteToBigQuery  
- Os nomes devem corresponder √†s chaves dos dicion√°rios gerados na transforma√ß√£o  
- Define a descri√ß√£o dos campos
```

#### 5.4 - Fun√ß√£o de transforma√ß√£o 
A fun√ß√£o  **`parse_and_transform()`** l√™ cada linha do CSV, aplica as fun√ß√µes auxiliares **`to_int`** e **`to_float`** para normalizar os tipos, e retorna um dicion√°rio pronto para ingest√£o no BigQuery.
(Lembram quando abrimos o arquivo csv para dar uma olhada? O campo Gross era um campo decimal mas era separado por v√≠rgula, ent√£o utilizamos a fun√ß√£o para retirar a virgula)


</div>
<img width="480" height="730" alt="Image" src="https://github.com/user-attachments/assets/8a711c69-b20a-4c7a-ba7b-543ac8ac557c" />
</div>


#### 5.5 - Adicionar descri√ß√£o na tabela
Fun√ß√£o para conectar na API do BigQuery e aplicar a descri√ß√£o da tabela.
A atualiza√ß√£o da descri√ß√£o √© feita via client da biblioteca oficial **`google-cloud-bigquery`**

</div>
<img width="874" height="218" alt="Image" src="https://github.com/user-attachments/assets/660717fc-fb4b-473e-b816-536a11918a1a" />
</div>


#### 5.6 - Pipeline Apache Beam
Aqui montamos o fluxo completo de leitura, transforma√ß√£o e escrita no BigQuery, usando o Apache Beam e executando no Google Dataflow.

</div>
<img width="672" height="482" alt="Image" src="https://github.com/user-attachments/assets/b949ff17-117d-4d70-9fdb-50ce8dea8650" />
</div>

```text
- job_name: o nome que o job ter√° no dataflow
- region: regi√£o que estamos utilizando
- temp_location: guarda arquivos tempor√°rios do job em execu√ß√£o. Essa pasta √© criada automaticamente
- staging_location: Guarda arquivos necess√°rios para iniciar o job. Essa pasta √© criada automaticamente
- ReadFromText: l√™ o CSV diretamente do GCS.
- beam.Map(parse_and_transform): aplica a fun√ß√£o de transforma√ß√£o linha a linha.
- WriteToBigQuery: grava os dados processados na tabela de destino.
Observa√ß√£o:
O modo WRITE_TRUNCATE substitui o conte√∫do da tabela a cada execu√ß√£o.
Use WRITE_APPEND caso queira apenas adicionar novos registros.
```


#### 5.7 - Execu√ß√£o
O pipeline Dataflow escreve os dados no BigQuery. Ao final, uma chamada √† API do BigQuery atualiza a descri√ß√£o da tabela para documentar seu conte√∫do.

</div>
<img width="744" height="276" alt="Image" src="https://github.com/user-attachments/assets/fb73d8e2-ed50-49d8-9ca4-8c3434ed002b" />
</div>


### Passo 6: Rodando o c√≥digo
Agora que j√° temos tudo configurado, podemos executar o c√≥digo no terminal

```bash
  python csv_dataflow_imdb.py
```

Ficar√° algo assim: 
</div>
<img width="1580" height="348" alt="Image" src="https://github.com/user-attachments/assets/53bd033a-3d3e-4059-b10c-4d3bdc6fe844" />
</div>

\
E quando voc√™ olhar no Dataflow, ver√° o seu job executando
</div>
<img width="1874" height="359" alt="Image" src="https://github.com/user-attachments/assets/f7383cf8-e7a1-4084-9567-d6575a0b24ce" />
</div>


Clicando nele, veremos as 3 etapas que criamos no Apache Beam
1. Ler CSV
2. Transformar registros
3. Escrever no BigQuery
   
</div>
<img width="1673" height="800" alt="Image" src="https://github.com/user-attachments/assets/6e054a26-e364-466d-809b-aed57ea563ca" />
</div>

\
O Job foi concl√∫ido com sucesso, agora vamos ver a tabela foi criado no BigQuery
<img width="1897" height="824" alt="Image" src="https://github.com/user-attachments/assets/20eb2906-c0ca-4120-87eb-53350495ee55" />



### Passo 7: Checkando os dados no BigQuery
Lembram que criamos o dataset no Passo 3?
Agora a nossa tabela foi materializada aqui 

</div>
<img width="342" height="808" alt="Image" src="https://github.com/user-attachments/assets/11939028-8928-4e3f-a383-b449a624e58f" />
</div>

\
Ao clicar nela, podemos ver alguns dados sobre ela:

No menu **`Esquema`** podemos ver o nome do campo, tipagem, descri√ß√£o de cada campo etc
<img width="1203" height="646" alt="Image" src="https://github.com/user-attachments/assets/4324b618-4534-402a-863d-7846eb4e25a2" />

Na aba **`Detalhes`** podemos ver alguns dados interessantes tamb√©m:

1. Informa√ß√µes da tabela:
Id da tabela, data de cria√ß√£o e data de modifica√ß√£o, descri√ß√£o da tabela etc
2. Informa√ß√µes de armazenamento:
N√∫mero de linhas e total de bytes l√≥gicos
</div>
<img width="1024" height="782" alt="Image" src="https://github.com/user-attachments/assets/9127e210-a530-492b-8e24-68c2b818fa54" />
</div>




\
E na aba **`Visualiza√ß√£o`** conseguimos ter uma pr√©via dos dados da tabela
</div>
<img width="1659" height="755" alt="Image" src="https://github.com/user-attachments/assets/d8c365d6-b0b4-406c-a250-9fac55c47128" />
</div>


\
Pra fazermos querys na tabela √© bem simples, podemos clicar em consulta e abrir√° uma nova aba 
</div>
<img width="1168" height="731" alt="Image" src="https://github.com/user-attachments/assets/aea3dccd-3a2c-4718-bc05-bfe1b5b236d6" />
</div>

### Passo 8: Fazendo consultas no BigQuery
#### 1 - Quais os 10 primeiros filmes com maiores notas no imdb?

```bash
select titulo, nota_imdb from `inbound-byway-475719-v0.imdb_dataset.raw_imdb` 
order by nota_imdb desc
limit 10
```
Obs: Antes de rodar a query o BigQuery j√° estima a quantidade de processamento que ser√° consumido ao executar a query
</div>
<img width="613" height="510" alt="Image" src="https://github.com/user-attachments/assets/505fda97-fb71-46a0-a461-5c4655cf87d4" />
</div>

#### 2 - Quais os nomes dos 10 primeiros filmes e suas respectivas notas que mais geraram receita?
```bash
select titulo, nota_imdb, receita from `inbound-byway-475719-v0.imdb_dataset.raw_imdb` 
order by receita desc
limit 10
```
</div>
<img width="813" height="478" alt="Image" src="https://github.com/user-attachments/assets/32a776d0-7e07-4c2c-af45-67df99129ee4" />
</div>

#### 3 - Quais s√£o os 5 anos mais recentes presentes na base e quantos t√≠tulos foram lan√ßados em cada um desses anos? (Lembrando que a base s√£o dos 1000 melhores filmes, ent√£o j√° possui um filtro)
```bash
select ano_lancamento, count(*) as qtd from `inbound-byway-475719-v0.imdb_dataset.raw_imdb` 
group by 1
order by 1 desc
limit 5
```
Obs: No BigQuery n√£o precisamos digitar o nome do campo no groupby e no orderby, podemos utilizar o n√∫mero conforme a posi√ß√£o que ele est√° na query
</div>
<img width="676" height="356" alt="Image" src="https://github.com/user-attachments/assets/63d34d9b-f53f-41b4-8876-c677bb6bf3e4" />
</div>

#### 4 - Quais os nomes e suas respectivas notas dos 10 primeiros filmes que mais geraram receita?
```bash
select titulo, nota_imdb, receita from `inbound-byway-475719-v0.imdb_dataset.raw_imdb` 
order by receita desc
limit 10
```
</div>
<img width="813" height="478" alt="Image" src="https://github.com/user-attachments/assets/32a776d0-7e07-4c2c-af45-67df99129ee4" />
</div>

#### 5 - Quais s√£o os 5 diretores com maior receita acumulada, e quantos filmes cada um deles possui?
```bash
select diretor, count(*) as qtd_filmes, sum(receita) as soma_receita from `inbound-byway-475719-v0.imdb_dataset.raw_imdb` 
group by 1
order by soma_receita desc
limit 5
```
</div>
<img width="884" height="378" alt="Image" src="https://github.com/user-attachments/assets/be19882b-596e-4755-89b4-f6d4c5b6d612" />
</div>

---

### Finalizado
E √© isso, pessoal!
Neste projeto eu quis demonstrar, de forma pr√°tica como construir um pipeline de ingest√£o de dados utilizando Google Cloud Storage, Apache Beam, Dataflow e BigQuery, desde o preparo do ambiente at√© a an√°lise dos dados ingeridos.

O objetivo foi mostrar um fluxo de ponta a ponta, que pode servir tanto como estudo quanto como base para pipelines mais robustos em ambientes de produ√ß√£o.
Se esse conte√∫do te ajudou ou te inspirou a criar algo parecido, j√° valeu o esfor√ßo! üòä
Fique √† vontade para tirar d√∫vidas e sugerir melhorias.

Obrigado por acompanhar at√© aqui e at√© o pr√≥ximo projeto! üòäüëä


### Pr√≥ximos passos??? Quem sabe üëÄ
O que pretendo adicionar na pr√≥xima vers√£o:
- Processamento e modelagem no Dataform
  - Vou criar camadas de transforma√ß√£o utilizando views e tabelas materializadas (staging ‚Üí intermediate ‚Üí mart) para organizar os dados de forma mais anal√≠tica.
- Cria√ß√£o de um dashboard no Looker Studio.
Ap√≥s modelar os dados no Dataform, vou construir um dashboard no Looker com m√©tricas como:
  - distribui√ß√£o de notas dos filmes
  - receita total por diretor
  - evolu√ß√£o temporal dos lan√ßamentos
  - indicadores sobre g√™neros, dura√ß√£o e popularidade


Essas etapas v√£o fechar o ciclo completo: ingest√£o ‚Üí transforma√ß√£o ‚Üí an√°lise visual.
